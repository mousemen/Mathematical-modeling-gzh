Topsis确定权重以及打分

import numpy as np
import warnings


class Topsis():
    evaluation_matrix = np.array([])  # Matrix
    weighted_normalized = np.array([])  # 权重矩阵
    normalized_decision = np.array([])  # 归一化矩阵
    M = 0  # 行数
    N = 0  # 维数

    '''
	创建由 m 个备选项和 n 个条件组成的评估矩阵，
每个备选项和条件的交集给出为 {\displaystyle x_{ij}}x_{ij}，
因此，我们有一个矩阵{\displaystyle（x_{ij}）_{m\times n}}（x_{{ij}}）_{{m\times n}}。
	'''

    def __init__(self, evaluation_matrix, weight_matrix, criteria):
        # M×N matrix
        self.evaluation_matrix = np.array(evaluation_matrix, dtype="float")

        # M alternatives (options)
        self.row_size = len(self.evaluation_matrix)

        # N attributes/criteria
        self.column_size = len(self.evaluation_matrix[0])

        # N size weight matrix
        self.weight_matrix = np.array(weight_matrix, dtype="float")
        self.weight_matrix = self.weight_matrix / sum(self.weight_matrix)
        self.criteria = np.array(criteria, dtype="float")

    '''
	# Step 2
	然后将矩阵 {\displaystyle （x_{ij}）_{m\times n}}（x_{{ij}}）_{{m\times n}} 归一化以形成矩阵
	'''

    def step_2(self):
        # normalized scores
        self.normalized_decision = np.copy(self.evaluation_matrix)
        sqrd_sum = np.zeros(self.column_size)
        for i in range(self.row_size):
            for j in range(self.column_size):
                sqrd_sum[j] += self.evaluation_matrix[i, j] ** 2
        for i in range(self.row_size):
            for j in range(self.column_size):
                self.normalized_decision[i,
                                         j] = self.evaluation_matrix[i, j] / (sqrd_sum[j] ** 0.5)

    '''
	# Step 3
	Calculate the weighted normalised decision matrix
	'''

    def step_3(self):
        from pdb import set_trace
        self.weighted_normalized = np.copy(self.normalized_decision)
        for i in range(self.row_size):
            for j in range(self.column_size):
                self.weighted_normalized[i, j] *= self.weight_matrix[j]

    '''
	# Step 4
	确定最劣的替代 {\displaystyle （A_{w}）}（A_{w}） 和最佳替代 {\displaystyle （A_{b}）}（A_{b}）：{b}):
	'''

    def step_4(self):
        self.worst_alternatives = np.zeros(self.column_size)
        self.best_alternatives = np.zeros(self.column_size)
        for i in range(self.column_size):
            if self.criteria[i]:
                self.worst_alternatives[i] = min(
                    self.weighted_normalized[:, i])
                self.best_alternatives[i] = max(self.weighted_normalized[:, i])
            else:
                self.worst_alternatives[i] = max(
                    self.weighted_normalized[:, i])
                self.best_alternatives[i] = min(self.weighted_normalized[:, i])

    '''
	# Step 5
	计算目标备选方案 {\displaystyle i}i 和最差条件 {\displaystyle A_{w}} A_{w} 之间的 L2 距离
{\displaystyle d_{iw}={\sqrt {\sum _{j=1}^{n}（t_{ij}-t_{wj}）^{2}}}，\quad i=1，2，\ldots ，m，}
以及备选 {\displaystyle i}i 与最佳条件 {\displaystyle A_{b}} 之间的距离A_b
{\displaystyle d_{ib}={\sqrt {\sum _{j=1}^{n}（t_{ij}-t_{bj}）^{2}}}，\quad i=1，2，\ldots ，m}
其中 {\displaystyle d_{iw}}d_{{iw}} 和 {\displaystyle d_{ib}}d_{{ib}} 是 L2-norm distances
从目标替代 {\displaystyle i}i 分别到最差和最佳条件。
	'''

    def step_5(self):
        self.worst_distance = np.zeros(self.row_size)
        self.best_distance = np.zeros(self.row_size)

        self.worst_distance_mat = np.copy(self.weighted_normalized)
        self.best_distance_mat = np.copy(self.weighted_normalized)

        for i in range(self.row_size):
            for j in range(self.column_size):
                self.worst_distance_mat[i][j] = (self.weighted_normalized[i][j] - self.worst_alternatives[j]) ** 2
                self.best_distance_mat[i][j] = (self.weighted_normalized[i][j] - self.best_alternatives[j]) ** 2

                self.worst_distance[i] += self.worst_distance_mat[i][j]
                self.best_distance[i] += self.best_distance_mat[i][j]

        for i in range(self.row_size):
            self.worst_distance[i] = self.worst_distance[i] ** 0.5
            self.best_distance[i] = self.best_distance[i] ** 0.5

    '''
	# Step 6
	计算相似度
	'''

    def step_6(self):
        np.seterr(all='ignore')
        self.worst_similarity = np.zeros(self.row_size)
        self.best_similarity = np.zeros(self.row_size)

        for i in range(self.row_size):
            # calculate the similarity to the worst condition
            self.worst_similarity[i] = self.worst_distance[i] / \
                                       (self.worst_distance[i] + self.best_distance[i])

            # calculate the similarity to the best condition
            self.best_similarity[i] = self.best_distance[i] / \
                                      (self.worst_distance[i] + self.best_distance[i])

    def ranking(self, data):
        return [i + 1 for i in data.argsort()]

    def rank_to_worst_similarity(self):
        # return rankdata(self.worst_similarity, method="min").astype(int)
        return self.ranking(self.worst_similarity)

    def rank_to_best_similarity(self):
        # return rankdata(self.best_similarity, method='min').astype(int)
        return self.ranking(self.best_similarity)

    def calc(self):
        print("Step 1\n", self.evaluation_matrix, end="\n\n")
        self.step_2()
        print("Step 2\n", self.normalized_decision, end="\n\n")
        self.step_3()
        print("Step 3\n", self.weighted_normalized, end="\n\n")
        self.step_4()
        print("Step 4\n", self.worst_alternatives,
              self.best_alternatives, end="\n\n")
        self.step_5()
        print("Step 5\n", self.worst_distance, self.best_distance, end="\n\n")
        self.step_6()
        print("Step 6\n", self.worst_similarity,
              self.best_similarity, end="\n\n")


——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————

层次分析法



import numpy as np
import pandas as pd
import warnings


class AHP:
    def __init__(self, criteria, b):
        self.RI = (0, 0, 0.58, 0.9, 1.12, 1.24, 1.32, 1.41, 1.45, 1.49)
        self.criteria = criteria
        self.b = b
        self.num_criteria = criteria.shape[0]
        self.num_project = b[0].shape[0]

    def cal_weights(self, input_matrix):
        input_matrix = np.array(input_matrix)
        n, n1 = input_matrix.shape
        assert n == n1, 'not a matrix'
        for i in range(n):
            for j in range(n):
                if np.abs(input_matrix[i, j] * input_matrix[j, i] - 1) > 1e-7:
                    raise ValueError('not a skew-symetric matrix')

        eigenvalues, eigenvectors = np.linalg.eig(input_matrix)

        max_idx = np.argmax(eigenvalues)
        max_eigen = eigenvalues[max_idx].real
        eigen = eigenvectors[:, max_idx].real
        eigen = eigen / eigen.sum()

        if n > 9:
            CR = None
            warnings.warn('consistency cannot be judged')
        else:
            CI = (max_eigen - n) / (n - 1)
            CR = CI / self.RI[n]
        return max_eigen, CR, eigen

    def run(self):
        max_eigen, CR, criteria_eigen = self.cal_weights(self.criteria)
        print('index layer：Maximum eigenvalue={:<5f},CR={:<5f},test{}passed'.format(max_eigen, CR, '' if CR < 0.1 else 'not'))
        print('index layer weight={}\n'.format(criteria_eigen))

        max_eigen_list, CR_list, eigen_list = [], [], []
        for i in self.b:
            max_eigen, CR, eigen = self.cal_weights(i)
            max_eigen_list.append(max_eigen)
            CR_list.append(CR)
            eigen_list.append(eigen)

        pd_print = pd.DataFrame(eigen_list,
                                index=['index' + str(i) for i in range(self.num_criteria)],
                                columns=['case' + str(i) for i in range(self.num_project)],
                                )
        pd_print.loc[:, 'max-eigenval'] = max_eigen_list
        pd_print.loc[:, 'CR'] = CR_list
        pd_print.loc[:, 'Consistency'] = pd_print.loc[:, 'CR'] < 0.1
        print('case layer')
        print(pd_print)

        # Target Layer
        obj = np.dot(criteria_eigen.reshape(1, -1), np.array(eigen_list))
        print('\ntarget layer', obj)
        print('The best choice is case{}'.format(np.argmax(obj)))
        return obj


if __name__ == '__main__':
    # Index Importance Matrix
    criteria = np.array([[1, 2, 7, 5, 5],
                         [1 / 2, 1, 4, 3, 3],
                         [1 / 7, 1 / 4, 1, 1 / 2, 1 / 3],
                         [1 / 5, 1 / 3, 2, 1, 1],
                         [1 / 5, 1 / 3, 3, 1, 1]])

    # For each index, the advantages and disadvantages of the scheme are ranked
    b1 = np.array([[1, 1 / 3, 1 / 8], [3, 1, 1 / 3], [8, 3, 1]])
    b2 = np.array([[1, 2, 5], [1 / 2, 1, 2], [1 / 5, 1 / 2, 1]])
    b3 = np.array([[1, 1, 3], [1, 1, 3], [1 / 3, 1 / 3, 1]])
    b4 = np.array([[1, 3, 4], [1 / 3, 1, 1], [1 / 4, 1, 1]])
    b5 = np.array([[1, 4, 1 / 2], [1 / 4, 1, 1 / 4], [2, 4, 1]])

    b = [b1, b2, b3, b4, b5]
    a = AHP(criteria, b).run()
    #If a negative number appears in CR in the calculation result, it is because the calculation result is very small, and there is a floating point error in the calculation process
————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————

秩和比评价法



import pandas as pd
import numpy as np
import statsmodels.api as sm
from scipy.stats import norm


def rsr(data, weight=None, threshold=None, full_rank=True):
	Result = pd.DataFrame()
	n, m = data.shape

	# Rank the original data
	if full_rank:
		for i, X in enumerate(data.columns):
			Result[f'X{str(i + 1)}：{X}'] = data.iloc[:, i]
			Result[f'R{str(i + 1)}：{X}'] = data.iloc[:, i].rank(method="dense")
	else:
		for i, X in enumerate(data.columns):
			Result[f'X{str(i + 1)}：{X}'] = data.iloc[:, i]
			Result[f'R{str(i + 1)}：{X}'] = 1 + (n - 1) * (data.iloc[:, i].max() - data.iloc[:, i]) / (data.iloc[:, i].max() - data.iloc[:, i].min())

	# Calculate rank sum ratio
	weight = 1 / m if weight is None else np.array(weight) / sum(weight)
	Result['RSR'] = (Result.iloc[:, 1::2] * weight).sum(axis=1) / n
	Result['RSR_Rank'] = Result['RSR'].rank(ascending=False)

	# Draw the RSR distribution table
	RSR = Result['RSR']
	RSR_RANK_DICT = dict(zip(RSR.values, RSR.rank().values))
	Distribution = pd.DataFrame(index=sorted(RSR.unique()))
	Distribution['f'] = RSR.value_counts().sort_index()
	Distribution['Σ f'] = Distribution['f'].cumsum()
	Distribution[r'\bar{R} f'] = [RSR_RANK_DICT[i] for i in Distribution.index]
	Distribution[r'\bar{R}/n*100%'] = Distribution[r'\bar{R} f'] / n
	Distribution.iat[-1, -1] = 1 - 1 / (4 * n)
	Distribution['Probit'] = 5 - norm.isf(Distribution.iloc[:, -1])

	# Calculate the regression variance and perform the regression analysis
	r0 = np.polyfit(Distribution['Probit'], Distribution.index, deg=1)
	print(sm.OLS(Distribution.index, sm.add_constant(Distribution['Probit'])).fit().summary())
	if r0[1] > 0:
		print(f"\nThe equation of the regression line is：y = {r0[0]} Probit + {r0[1]}")
	else:
		print(f"\nThe equation of the regression line is：y = {r0[0]} Probit - {abs(r0[1])}")

	# Substitute it into the regression equation and sort it
	Result['Probit'] = Result['RSR'].apply(lambda item: Distribution.at[item, 'Probit'])
	Result['RSR Regression'] = np.polyval(r0, Result['Probit'])
	threshold = np.polyval(r0, [2, 4, 6, 8]) if threshold is None else np.polyval(r0, threshold)
	Result['Level'] = pd.cut(Result['RSR Regression'], threshold, labels=range(len(threshold) - 1, 0, -1))

	return Result, Distribution


def rsrAnalysis(data, file_name=None, **kwargs):
	Result, Distribution = rsr(data, **kwargs)
	file_name = 'RSR analysis results report.xlsx' if file_name is None else file_name + '.xlsx'
	Excel_Writer = pd.ExcelWriter(file_name)
	Result.to_excel(Excel_Writer, 'Comprehensive evaluation results')
	Result.sort_values(by='Level', ascending=False).to_excel(Excel_Writer, 'Sort results')
	Distribution.to_excel(Excel_Writer, 'RSR distribution table')
	Excel_Writer.save()

	return Result, Distribution


data = pd.DataFrame({'Antenatal screening rate': [99.54, 96.52, 99.36, 92.83, 91.71, 95.35, 96.09, 99.27, 94.76, 84.80],
                     'Maternal mortality rate': [60.27, 59.67, 43.91, 58.99, 35.40, 44.71, 49.81, 31.69, 22.91, 81.49],
                     'Perinatal mortality': [16.15, 20.10, 15.60, 17.04, 15.01, 13.93, 17.43, 13.89, 19.87, 23.63]},
                    index=list('ABCDEFGHIJ'), columns=['Antenatal screening rate', 'Maternal mortality rate', 'Perinatal mortality'])
data["Maternal mortality rate"] = 1 / data["Maternal mortality rate"]
data["Perinatal mortality"] = 1 / data["Perinatal mortality"]
rsr(data)



————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————

熵权法



import pandas as pd
import numpy as np
import math
from numpy import array
 
# Read the data
df = pd.read_csv('ewm.csv', encoding='gb2312')
# Preliminary data, remove records with null values
df.dropna()
 
#Define the EWM function
def cal_weight(x):
    '''calculate the weight of variables'''
    # standardized
    x = x.apply(lambda x: ((x - np.min(x)) / (np.max(x) - np.min(x))))
 
    # Calculate k
    rows = x.index.size  # row
    cols = x.columns.size  # column
    k = 1.0 / math.log(rows)
 
    lnf = [[None] * cols for i in range(rows)]
 
    # Matrix calculation
    # Information entropy
    # p=array(p)
    x = array(x)
    lnf = [[None] * cols for i in range(rows)]
    lnf = array(lnf)
    for i in range(0, rows):
        for j in range(0, cols):
            if x[i][j] == 0:
                lnfij = 0.0
            else:
                p = x[i][j] / x.sum(axis=0)[j]
                lnfij = math.log(p) * p * (-k)
            lnf[i][j] = lnfij
    lnf = pd.DataFrame(lnf)
    E = lnf
 
    # Calculate redundancy
    d = 1 - E.sum(axis=0)
    # Calculate the weight of each indicator
    w = [[None] * 1 for i in range(cols)]
    for j in range(0, cols):
        wj = d[j] / sum(d)
        w[j] = wj
        # Calculate the comprehensive score of each sample, using the most original data
    
    w = pd.DataFrame(w)
    return w
 
 
if __name__ == '__main__':
    # Calculate the weight of each field of df
    w = cal_weight(df)  # Call cal_weight
    w.index = df.columns
    w.columns = ['weight']
    print(w)
    print('finish!')
 
——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————
